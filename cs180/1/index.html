<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="styles.css" />
  <title>CS180 - Project 1</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 20px;
    }
  </style>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>Table template</title>
  <link href="minimal-table.css" rel="stylesheet" type="text/css">
</head>
<body>
  <h1>Project 1: Images of the Russian Empire -- Colorizing the Prokudin-Gorskii Photo Collection</h1>
  <h2> Part 1: Single-Scale Alignment</h2>
  <p>I started working off of the provided starter code, which split the source images into
    blue, green, and red channels based on their vertical positions on the image. For the green
    and red channels, I tried aligning them to the blue channel by testing over a window of 
    possible displacements that was [-30, 30] in both the horizontal and vertical directions.
    My algorithm exhaustively searched all 61 x 61 possible displacements in this window,
    and chose the displacement with the highest similarity among RGB values.
    After the red and green channels are aligned, the three channels are cropped to their
    shared area.
    <br><br>
    For comparing similarity, I used the Normalized Cross-Correlation, and implemented the
    translations by cropping by the source and reference images to their shared areas,
    instead of calling np.roll on the source image. Normalized Cross-Correlation is only
    computed for pixels in the 10th to 90th percentiles on both axes. In other words, 
    I cropped out 10% of the pixels on the four edges to avoid having the borders
    influence NCC calculations.
    <br><br>
    Formally, the NCC metric I used was the sum over all values in the pointwise product
    of the two image matrices, then dividing it by the Frobenius norms of the two matrices.
    Normalizing is particularly relevant in my approach because cropping (instead
    of rolling) images means that
    larger translations result in smaller matrices, which tend to have smaller norms.
    <br><br>
    The results of this algorithm on small images is shown below. Note that the green
    and red channel offsets refer to the displacement of those channels relative to
    the blue channel. An offset of <i>(a, b)</i> refers to displacing that channel by 
    <i>a</i> pixels vertically and <i>b</i> pixels horizontally.
  </p>
  <Table>
    <tr>
      <td>Image</td>
      <td>Green Channel Offset</td>
      <td>Red Channel Offset</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/cathedral.jpg"
        alt="Cathedral."
      ></td>
      <td>(5, 2)</td>
      <td>(12, 3)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/monastery.jpg"
        alt="Monastery."
      ></td>
      <td>(-3, 2)</td>
      <td>(3, 2)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/tobolsk.jpg"
        alt="Tobolsk."
      ></td>
      <td>(3, 3)</td>
      <td>(6, 3)</td>
    </tr>
  </Table>
  <h2> Part 2: Multi-Scale Pyramid Alignment</h2>
  <p>While single scale alignment works sufficiently for smaller images, it is too inefficient
    for exhaustively searching large images. The true displacements of channels in large
    image may be much larger than those of small images, since the same number of pixels is 
    a smaller fraction of the overall image. For instance, if a channel needed to be displaced
    to the right by 10% of the width of the image, it would be displaced by 10 pixels in an 
    image with a width of 100 pixels, but 100 pixels on an image with a width of 1000 pixels.
    <br><br>
    To avoid exhaustively searching a signficantly larger window of displacements, I used an
    image pyramid to reduce the number of displacements that need to be considered. The image
    is first iteratively downsampled by factors of 2 (with anti-aliasing) with the
    sk.transorm.rescale function until the image is at most 600 pixels in both the 
    horizontal and vertical dimensions. On this image, the previous window of [-30, 30]
    is used to find the displacement with highest similarity.
    <br><br>
    In each subsequent (less-downsampled) layer, my algorithm searches the 2x2 window that
    represents the best displacement found on the previous iteration. The window is 2x2
    since the downsampling was done with a factor of 2, so after upscaling, each pixel is 
    split into 4.
    <br><br>
    The results of using the pyramid algorithm on the other example images, as well
    as 3 additional images I downloaded from the Library of Congress, are shown below. The
    results of this procedure on the smaller images is ommitted from this section since those
    images already begin with dimensions less than 600 pixels on each side, meaning the pyramid
    algorithm is identical to the single-scale alignment. The images are downsized to fit on
    the webpage, but you may open the images in a new tab to see them at full scale.
  </p>
  <Table>
    <tr>
      <td>Image</td>
      <td>Green Channel Offset</td>
      <td>Red Channel Offset</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/church.jpg"
        alt="Church."
        width="800"
      ></td>
      <td>(25, 4)</td>
      <td>(60, -4)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/emir.jpg"
        alt="Emir."
        width="800"
      ></td>
      <td>(50, 24)</td>
      <td>(103, 56)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/harvesters.jpg"
        alt="Harvesters."
        width="800"
      ></td>
      <td>(60, 18)</td>
      <td>(128, 16)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/icon.jpg"
        alt="Icon."
        width="800"
      ></td>
      <td>(44, 17)</td>
      <td>(92, 24)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/italil.jpg"
        alt="Italil."
        width="800"
      ></td>
      <td>(40, 24)</td>
      <td>(80, 36)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/lastochikino.jpg"
        alt="lastochikino."
        width="800"
      ></td>
      <td>(0, 0)</td>
      <td>(76, -8)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/lugano.jpg"
        alt="Lugano."
        width="800"
      ></td>
      <td>(41, -16)</td>
      <td>(96, -28)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/melons.jpg"
        alt="Melons."
        width="800"
      ></td>
      <td>(80, 10)</td>
      <td>(180, 16)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/self_portrait.jpg"
        alt="Self Portrait."
        width="800"
      ></td>
      <td>(80, 32)</td>
      <td>(178, 40)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/siren.jpg"
        alt="Siren."
        width="800"
      ></td>
      <td>(49, -6)</td>
      <td>(96, -24)</td>
    </tr>

    <tr>
      <td><img
        src="base_outputs/three_generations.jpg"
        alt="Three Generations."
        width="800"
      ></td>
      <td>(56, 16)</td>
      <td>(114, 12)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/column.jpg"
        alt="Column."
        width="800"
      ></td>
      <td>(12, 20)</td>
      <td>(65, 15)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/grebeshki.jpg"
        alt="Grebeshki."
        width="800"
      ></td>
      <td>(54, 16)</td>
      <td>(122, -6)</td>
    </tr>
    <tr>
      <td><img
        src="base_outputs/perm.jpg"
        alt="Perm."
        width="800"
      ></td>
      <td>(58, 10)</td>
      <td>(128, 24)</td>
    </tr>
  </Table>
  <p>The search with NCC using the pyramid algorithm appears to have worked reasonably well
    for most images above. Notably, multiple images had offsets of around 180 for one channel
    in one direction (like self portrait and melons), which would be difficult to 
    search for through brute force. I had previously had trouble aligning these images
    because the window of displacements I searched over was too small ([-15, 15]).
    <br><br>
    The image it aligned poorly is the one with the emir, the second
    image on the table above. This may be a shortcoming of directly comparing RGB values.
    From the original scans, it is clear that the emir's clothing has much higher intensity
    in the blue channel compared to the red channel, so those channels are not very similar
    by many similarity metrics. A fix for this will be discussed in the Bells and Whistles.
  </p>
  <h2>Part 3: Bells and Whistles</h2>
  <p>These are optional components of the project, but I implemented some to
    improve image quality.
  </p>
  <h3>Automatic Cropping</h3>
    <p> I noticed that many of the borders in the images are straight, and either black
    or white. Thus, I wrote a simple algorithm to detect some of these edges so they
    can be truncated. More formally, the average intensities along each row and column of
    each channel (after being converted to floats in [0.0, 1.0]) are taken. Rows and columns
    with an average intensity below 8.0/256.0 are considered very low intensity, 
    and those with an average intensity above 248.0/256.0 are considered to have
    very high intensity. Among the rows and columns with very low or high intensity, my
    algorithm finds the two consecutive rows or columns that have the greatest distance in
    terms of their positions and crops them.
    <br><br>
    For instance, if rows [0, 1, 3, 5, 100, 103] are detected
    to have very high or low intensity, the image is cropped to rows 6 to 99, inclusive.
    Since this is done for each channel, the common area among the channels is found, and
    each channel is cropped to this common area (to keep them the same shape).
    <br><br>
    A sample crop is shown below. This is the result of cropping the blue channel
    for the cathedral picture, and after
    all channels are cropped to common area. The image before cropping
    is on the left, and the image after cropping is on the right. The right black border
    been cropped slightly. White borders have also been cropped, which
    are more noticeable on color images (since white doesn't show up
    on this webpage).</p>
    <br><br>
    <div class="container">
      <img
        src="other_outputs/cathedral_b_old.jpg"
        alt="Old cutout."
        width="400"
      >
      <img
        src="other_outputs/cathedral_b_new.jpg"
        alt="Cropped cutout."
        width="400"
      >
    </div>
    <p>While this cropping algorithm does not crop all borders, it is able to
      crop some borders, without cropping any relevant parts of the photos.
    </p>
    <h3>Automatic Color Balance</h3>
    <p>I had initially tried automatic contrasting by scaling all pixel values 
      according to the linear function that maps the darkest pixel on the darkest channel
      to 0.0, and the brightest pixel on the brightest channel to 1.0. Formally, the
      linear function to do this is to subtract the darkest pixel's intensity from all
      pixels, then divide all pixels by the brightest pixel's value. However, I found that
      the darkest pixel was often close to 0.0 (on its darkest channel) 
      and the brightest pixel (on its brightest channel) was often close to 1.0, so this
      function did little to the image, even when only searching the interior of the
      image for the darkest and brightest pixel.
    <br><br>
      A similar, but more drastic change came when I tried automatic color balancing. I
      chose the "brightest" and "darkest" pixels in the interior of the image based on
      which had the highest or lowest sum of intensities among its three channels. Then,
      for each channel, I would subtract the "darkest" pixel's intensities from each channel
      to set to pixel to black, then divide by the "brightest" pixel's intensities to
      set that pixel to white. This is done similarly to the contrasting algorithm, but
      is done per channel. Since this can induce values outside the range of [0.0, 1.0],
      I rounded negative values to 0.0 and values above 1.0 to 1.0.
      <br><br>
      This resulted in more significant changes of some of the images that had poor lighting.
      For instance, the lastochikino image changed significantly after color balancing,
      as shown below. The original image is on the left, and the color-balanced one
      is on the right.
    </p>
    <div class="container">
      <img
        src="other_outputs/lastochikino_old.jpg"
        alt="Old image."
        width="400"
      >
      <img
        src="final_outputs/lastochikino.jpg"
        alt="Balanced image."
        width="400"
      >
    </div>
    <h3>Gradient Features</h3>
    I tried using gradients instead of intensity values to align the images instead.
    I used a Sobel filter (with padding that copied the border values) to get the gradient
    values.
    This worked particularly well for the emir image, where the person's clothing
    had significantly different intensities on different channels due to its color.
    This caused similarity among channels intensities to be unreliable for
    alignment.
    The image below on the left is the red channel of the emir image, and 
    the image on the right is its gradients.
    <div class="container">
      <img
        src="other_outputs/emir_r.jpg"
        alt="Old emir red."
        width="400"
      >
      <img
        src="other_outputs/emir_r_grad.jpg"
        alt="Emir red gradients."
        width="400"
      >
    </div>
    Overall, using gradients helped with aligning the image better. The left image
    below is the result of aligning with intensity values, and the image on the right
    is the result of aligning on gradients (both with NCC and the pyramid).
    <div class="container">
      <img
        src="other_outputs/emir_old.jpg"
        alt="Old emir red."
        width="400"
      >
      <img
        src="final_outputs/emir.jpg"
        alt="Emir red gradients."
        width="400"
      >
    </div>
    <h3>Final Results</h3>
    The table below contains the results of my alignment algorithm on my final settings.
    This uses the pyramid algorithm described in part 2, as well as automatic cropping,
    gradient features, and automatic color balancing. The images are resized to 
    fit on this webpage, but they can be viewed in full resolution in another tab.
    <Table>
    <tr>
      <td>Image</td>
      <td>Green Channel Offset</td>
      <td>Red Channel Offset</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/cathedral.jpg"
        alt="Cathedral."
      ></td>
      <td>(5, 2)</td>
      <td>(12, 3)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/monastery.jpg"
        alt="Monastery."
      ></td>
      <td>(-3, 2)</td>
      <td>(3, 2)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/tobolsk.jpg"
        alt="Tobolsk."
      ></td>
      <td>(3, 3)</td>
      <td>(6, 3)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/church.jpg"
        alt="Church."
        width="800"
      ></td>
      <td>(26, 8)</td>
      <td>(60, -4)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/emir.jpg"
        alt="Emir."
        width="800"
      ></td>
      <td>(50, 24)</td>
      <td>(108, 40)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/harvesters.jpg"
        alt="Harvesters."
        width="800"
      ></td>
      <td>(60, 18)</td>
      <td>(128, 16)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/icon.jpg"
        alt="Icon."
        width="800"
      ></td>
      <td>(44, 17)</td>
      <td>(92, 24)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/italil.jpg"
        alt="Italil."
        width="800"
      ></td>
      <td>(40, 24)</td>
      <td>(80, 36)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/lastochikino.jpg"
        alt="lastochikino."
        width="800"
      ></td>
      <td>(0, 0)</td>
      <td>(76, -8)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/lugano.jpg"
        alt="Lugano."
        width="800"
      ></td>
      <td>(41, -16)</td>
      <td>(96, -28)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/melons.jpg"
        alt="Melons."
        width="800"
      ></td>
      <td>(80, 10)</td>
      <td>(180, 12)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/self_portrait.jpg"
        alt="Self Portrait."
        width="800"
      ></td>
      <td>(80, 32)</td>
      <td>(178, 40)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/siren.jpg"
        alt="Siren."
        width="800"
      ></td>
      <td>(50, -6)</td>
      <td>(96, -24)</td>
    </tr>

    <tr>
      <td><img
        src="final_outputs/three_generations.jpg"
        alt="Three Generations."
        width="800"
      ></td>
      <td>(56, 16)</td>
      <td>(113, 12)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/column.jpg"
        alt="Column."
        width="800"
      ></td>
      <td>(12, 20)</td>
      <td>(65, 16)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/grebeshki.jpg"
        alt="Grebeshki."
        width="800"
      ></td>
      <td>(54, 16)</td>
      <td>(122, -6)</td>
    </tr>
    <tr>
      <td><img
        src="final_outputs/perm.jpg"
        alt="Perm."
        width="800"
      ></td>
      <td>(57, 10)</td>
      <td>(128, 24)</td>
    </tr>
  </Table>
</body>
</html>
